========================================
ETF Atlas 발표 스크립트
- Apache AGE & smolagents 기술 소개와 ETF 정보 서비스 적용기
========================================


========================================
오프닝
========================================

안녕하세요. 오늘은 리서치 프로젝트로 진행한 ETF Atlas를 소개하면서,
그 과정에서 사용한 기술들에 대해 이야기하려 합니다.

--- 문제 제기 ---

본격적인 기술 소개에 앞서, 왜 이런 기술이 필요한지부터 짚어보겠습니다.

한국 ETF가 약 900개인데, 그중 순자산 기준으로 걸러낸
우리 유니버스만 해도 수백 개이고, 보유종목이 수천 개입니다.
이 데이터에서 의미 있는 정보를 뽑으려면 관계를 탐색해야 합니다.

예를 들어 "삼성전자를 가장 많이 보유한 ETF는?" 이라는 질문은
관계형 DB에서 JOIN 3개에 GROUP BY, ORDER BY를 써야 합니다.
"KODEX 200과 비슷한 ETF는?"은 자기 조인에 서브쿼리, HAVING까지 필요합니다.
쿼리가 복잡해질수록 성능과 가독성 모두 떨어집니다.

거기에 한 가지 더 욕심을 내봤습니다.
이런 복잡한 질문을 개발자가 아닌 일반 사용자도 할 수 있으면 어떨까?
"반도체 ETF 중 보수율 낮은 것 알려줘"라고 자연어로 물어보면
알아서 데이터를 찾아 답해주는 챗봇을 만들고 싶었습니다.

이 두 가지 — 복잡한 관계 탐색과 자연어 질의 — 를
해결하기 위해 선택한 기술이 Apache AGE와 smolagents입니다.
여기에 데이터 수집 자동화를 위한 Apache Airflow까지,
이 세 가지 기술의 실제 적용 경험을 공유하겠습니다.

--- 기술 선택 배경 ---

이 프로젝트는 회사에서 기술 리서치 차원으로 진행했습니다.
주제는 평소 관심이 있던 ETF 투자로 잡았는데,
실제 서비스에 가까운 도메인으로 기술을 검증해야
리서치 결과가 의미 있다고 생각했기 때문입니다.

리서치 대상 기술은 세 가지입니다.

첫 번째는 그래프 데이터베이스입니다.
ETF 데이터는 본질적으로 관계 중심입니다.
ETF가 어떤 종목을 보유하고, 종목은 또 다른 ETF에 포함되고,
이런 관계를 표현하는 데 그래프 DB가 적합합니다.
Neo4j가 가장 유명하지만 라이선스 제약이 있어서,
PostgreSQL 확장으로 동작하는 Apache AGE를 선택했습니다.
자세한 비교는 뒤에서 다루겠습니다.

두 번째는 LLM 에이전트 프레임워크입니다.
챗봇 기능을 넣고 싶었는데, LangChain은 추상화가 무거워서
경량 프레임워크인 smolagents를 선택했습니다.

세 번째는 데이터 파이프라인입니다.
매일 KRX에서 ETF 데이터를 수집하고, 가공하고, 적재해야 하는데,
배치 중심 워크로드에 Python 네이티브인 Apache Airflow를 선택했습니다.

이 세 기술이 실제 서비스 수준의 프로젝트에서 얼마나 쓸 만한지
직접 검증해 보는 것이 이번 리서치의 목표였습니다.


========================================
Apache AGE 기술 소개
========================================

--- Apache AGE란? ---

Apache AGE는 Apache Software Foundation의 인큐베이팅 프로젝트로,
PostgreSQL에 그래프 데이터베이스 기능을 추가하는 확장(Extension)입니다.
이름 자체가 "A Graph Extension"의 약자입니다.

핵심 특징은 세 가지입니다.
첫째, PostgreSQL 위에서 동작합니다. 별도 DB를 띄울 필요 없이 확장만 설치하면 됩니다.
둘째, Cypher 쿼리를 지원합니다. Neo4j에서 쓰던 그 Cypher 문법 그대로 쓸 수 있습니다.
셋째, SQL과 Cypher를 함께 쓸 수 있습니다.
기존 관계형 테이블과 그래프 데이터를 하나의 DB에서 조회할 수 있다는 게 큰 장점입니다.

라이선스는 Apache 2.0으로, 상용 환경에서도 자유롭게 사용 가능합니다.

--- Neo4j vs Apache AGE ---

Neo4j와 비교하면 이렇습니다.

Neo4j는 전용 그래프 DB입니다. 성능과 기능 면에서 가장 성숙합니다.
하지만 Community Edition은 AGPL, Enterprise는 상용 라이선스입니다.
별도의 서버를 운영해야 하고, 기존 RDBMS와 데이터를 분리 관리해야 합니다.

Apache AGE는 PostgreSQL 확장입니다.
기존 PostgreSQL 인프라를 그대로 활용하면서 그래프 기능을 추가합니다.
하나의 DB에서 관계형 데이터와 그래프 데이터를 모두 다룰 수 있습니다.
다만 Neo4j 대비 Cypher 지원이 완전하지 않고, 에코시스템이 작습니다.
성능도 대규모 그래프 순회에서는 Neo4j가 우위입니다.

정리하면, 이미 PostgreSQL을 쓰고 있고,
그래프 쿼리가 핵심 워크로드의 일부라면 AGE가 합리적인 선택입니다.

--- AGE 동작 방식 ---

AGE에서 Cypher 쿼리를 실행하는 방식을 보겠습니다.

일반 SQL 안에 cypher() 함수를 호출하는 형태입니다.

  SELECT * FROM cypher('etf_graph', $$
      MATCH (e:ETF)-[h:HOLDS]->(s:Stock)
      WHERE e.code = '069500'
      RETURN {stock: s.name, weight: h.weight}
  $$) as (result agtype);

여기서 주의할 점이 있습니다.
첫째, 그래프 이름을 지정해야 합니다. 여기서는 'etf_graph'입니다.
둘째, 결과 타입이 agtype이라는 AGE 전용 타입입니다.
셋째, RETURN 값이 여러 개일 때는 맵으로 감싸야 합니다.
단일 컬럼으로만 반환되기 때문에, {key: value} 형태로 묶어서 내보내야 합니다.

이런 부분들이 Neo4j와는 다른 AGE만의 특성입니다.


========================================
smolagents 기술 소개
========================================

--- smolagents란? ---

smolagents는 Hugging Face에서 만든 경량 AI 에이전트 프레임워크입니다.
2024년 말에 공개됐고, 핵심 철학은 "최소한의 추상화"입니다.

LangChain이 모든 것을 추상화하려는 접근이라면,
smolagents는 필요한 것만 제공하고 나머지는 개발자가 직접 제어하는 방식입니다.

핵심 구성 요소는 세 가지입니다.

첫째, Model입니다.
에이전트의 두뇌 역할을 하는 LLM입니다.
OpenAI, Anthropic, Hugging Face 모델 등 다양한 모델을 플러그인 방식으로 연결할 수 있습니다.

둘째, Tool입니다.
에이전트가 실행할 수 있는 도구입니다.
Tool 클래스를 상속받아 name, description, inputs, output_type, forward() 다섯 가지만 정의하면 됩니다.
에이전트는 description을 보고 어떤 도구를 호출할지 판단합니다.

셋째, Agent입니다.
Model과 Tool을 조합해서 사용자 질문에 답하는 실행 주체입니다.
smolagents의 가장 큰 특징은 CodeAgent인데,
일반적인 에이전트가 JSON으로 도구 호출을 결정하는 반면,
CodeAgent는 Python 코드를 직접 생성해서 도구를 호출합니다.
변수에 값을 저장하고, 조건 분기도 가능해서
도구 간 데이터 전달이 자연스럽습니다.

정리하면, Model + Tool + Agent, 이 세 가지가 전부입니다.

--- LangChain vs smolagents ---

두 프레임워크를 비교하겠습니다.

LangChain은 풍부한 에코시스템이 강점입니다.
수백 개의 통합, 문서화, 커뮤니티가 잘 갖춰져 있습니다.
하지만 추상화 레이어가 깊어서 디버깅이 어렵고,
간단한 기능도 여러 클래스를 거쳐야 하는 경우가 많습니다.
버전 업데이트 시 breaking change도 잦습니다.

smolagents는 반대입니다.
코어가 작고, 코드를 직접 읽을 수 있을 만큼 단순합니다.
Tool 클래스 하나, Agent 클래스 하나면 시작할 수 있습니다.
다만 에코시스템이 작고, 고급 기능(메모리, RAG 체인 등)은 직접 구현해야 합니다.

도구 호출 중심의 에이전트를 만든다면
smolagents가 훨씬 빠르게 프로토타이핑할 수 있었습니다.


========================================
Apache Airflow 기술 소개
========================================

--- Apache Airflow란? ---

이 프로젝트에서 데이터 수집 파이프라인을 만들면서 처음으로 Airflow를 사용했습니다.
그 경험을 공유하겠습니다.

Apache Airflow는 데이터 파이프라인을 작성하고, 스케줄링하고, 모니터링하는 플랫폼입니다.
Airbnb에서 2014년에 만들었고, 지금은 Apache 최상위 프로젝트입니다.

핵심 개념은 네 가지입니다.

첫째, DAG입니다.
Directed Acyclic Graph, 방향이 있고 순환이 없는 그래프라는 뜻인데,
쉽게 말하면 "작업 흐름을 코드로 정의한 것"입니다.
어떤 작업이 어떤 순서로 실행되어야 하는지를 Python 코드로 작성합니다.

둘째, Task입니다.
DAG 안에서 실제로 실행되는 개별 작업 단위입니다.
"ETF 데이터 수집", "보유종목 파싱", "수익률 계산" 같은 것들이 각각 하나의 Task입니다.

셋째, Operator입니다.
Task가 "무엇을 할지" 정의하는 템플릿입니다.
Python 함수를 실행하는 PythonOperator, 조건에 따라 다음 작업을 건너뛰는 ShortCircuitOperator 등
다양한 종류가 있습니다.

넷째, Scheduler입니다.
DAG의 스케줄에 따라 작업을 자동으로 트리거하는 엔진입니다.
"평일 오전 8시마다", "매주 일요일 새벽 2시마다" 같은 주기를 설정하면
Scheduler가 알아서 실행합니다.

--- 왜 Airflow를 선택했나 — NiFi와의 비교 ---

NiFi는 NSA에서 만든 데이터 흐름 관리 시스템으로,
웹 브라우저에서 드래그 앤 드롭으로 파이프라인을 구성하는 것이 특징입니다.

두 도구의 철학이 상당히 다릅니다.

NiFi는 "실시간 데이터 라우팅"에 초점을 둡니다.
데이터가 들어오면 즉시 변환하고 전달하는, 스트리밍에 가까운 방식입니다.
GUI로 프로세서를 연결해서 플로우를 만들고,
데이터의 출처(provenance)를 자동으로 추적합니다.
IoT 센서 데이터, 로그 수집, 시스템 간 실시간 데이터 전달에 강합니다.

Airflow는 "배치 작업 오케스트레이션"에 초점을 둡니다.
정해진 시간에 작업을 실행하고, 작업 간 의존 관계를 관리합니다.
Python 코드로 워크플로우를 정의하기 때문에
버전 관리가 되고, 코드 리뷰도 가능합니다.
배치 ETL, 정기 데이터 수집, ML 파이프라인에 강합니다.

ETF Atlas의 요구사항을 보면 답이 나옵니다.
"평일 장 마감 후에 KRX에서 데이터를 가져와서 가공한 뒤 DB에 적재한다."
전형적인 배치 작업입니다.

또 하나 결정적이었던 건 코드 기반이라는 점입니다.
NiFi는 GUI에서 플로우를 만들기 때문에 설정이 XML/JSON으로 저장됩니다.
Git으로 버전 관리를 하려면 별도의 레지스트리 설정이 필요합니다.
Airflow는 DAG 자체가 Python 파일이니까
다른 코드와 동일하게 Git으로 관리할 수 있습니다.
이 프로젝트처럼 수집 로직에 pykrx, yfinance 같은 Python 라이브러리를
직접 호출해야 하는 경우에는 Python 네이티브인 Airflow가 자연스럽습니다.

정리하면, 실시간 데이터 라우팅이 필요하면 NiFi,
배치 작업의 스케줄링과 의존성 관리가 필요하면 Airflow를 추천합니다.

========================================
ETF Atlas 프로젝트 적용 사례
========================================

--- 프로젝트 개요 ---

ETF Atlas는 한국 ETF 정보를 수집하고, 분석하고, AI 챗봇으로 질의할 수 있는 서비스입니다.

주요 기능은 네 가지입니다.
하나, ETF 탐색 — 테마별 분류, 검색, 유사 ETF 찾기
둘, 포트폴리오 관리 — ETF/주식 매수 기록, 목표 비중 설정, 수익률 추적
셋, 워치리스트 — 관심 ETF 등록, 보유종목 변동 알림
넷, AI 챗봇 — 자연어로 ETF 데이터 질의

관심 있는 도메인에 리서치 대상 기술을 적용해서 실전 수준으로 검증한 프로젝트입니다.

--- 전체 아키텍처 ---

시스템은 4개의 Docker 컨테이너로 구성됩니다.

- DB: PostgreSQL 17 + Apache AGE + pgvector
- Backend: FastAPI (Python) + smolagents
- Frontend: React + TypeScript + Vite
- Airflow: 데이터 수집 파이프라인

하나의 PostgreSQL에 관계형 테이블과 그래프 데이터가 공존합니다.
사용자 인증, 포트폴리오 같은 CRUD는 관계형 테이블을 쓰고,
ETF-종목 관계, 가격 이력, 태그 같은 관계 탐색은 그래프를 씁니다.

이게 Apache AGE의 가장 큰 장점입니다.
별도의 그래프 DB 서버 없이, 하나의 DB에서 두 가지 패러다임을 활용합니다.

--- 데이터 파이프라인 — Airflow 활용 ---

데이터가 어떻게 수집되고 흘러가는지 보겠습니다.

ETF Atlas에서는 5개의 Airflow DAG를 운영하고 있습니다.

첫 번째, 일별 ETF 동기화입니다. (AGE)
거래일 다음날 새벽 4시(KST)에 자동으로 실행됩니다.
KRX API에서 전일 ETF 거래 데이터를 가져오고,
ETF 노드와 Price 노드를 생성하고,
pykrx로 보유종목 PDF를 파싱해서 HOLDS 관계를 만듭니다.
마지막으로 수익률을 계산하고, 보유종목 변동이 크면 Discord로 알림을 보냅니다.

두 번째, 실시간 가격 수집입니다. (RDB)
평일 장중에 10분마다 실행됩니다.
포트폴리오에 담긴 종목들의 현재 가격을 yfinance로 가져와서
포트폴리오 스냅샷을 업데이트합니다.

여기서 ShortCircuitOperator를 활용했습니다.
첫 번째 Task가 "지금 장이 열려 있는가?"를 판단합니다.
오늘이 거래일인지, 현재 시각이 09시~15시 30분 사이인지,
이미 장 마감 후 가격을 수집했는지를 확인합니다.
조건에 맞지 않으면 이후 Task를 모두 건너뜁니다.
10분마다 실행되지만 불필요한 API 호출은 하지 않는 겁니다.

세 번째, 주간 태깅입니다. (AGE)
토요일 새벽 3시에 전체 ETF에 테마 태그를 재할당합니다.
규칙 기반, 키워드 매칭, LLM 분류의 3단계 전략을 씁니다.

네 번째, RDB 메타데이터 동기화입니다. (RDB)
평일 오전 7시에 실행되는 가벼운 DAG입니다.
KRX에서 ETF 코드와 이름만 가져와서 관계형 테이블에 동기화합니다.

다섯 번째, 백필입니다. (AGE)
수동으로 실행하는 초기 데이터 적재용 DAG입니다.
과거 거래일 데이터를 한꺼번에 수집합니다.
보유종목 수집에만 6시간 타임아웃을 걸어둘 정도로 시간이 오래 걸립니다.

전체 데이터 흐름을 정리하면,
KRX API와 pykrx에서 원시 데이터를 수집하고,
Airflow DAG가 이를 가공해서 PostgreSQL에 적재합니다.
AGE 그래프, RDB 테이블, pgvector 임베딩이 모두 같은 DB에 들어갑니다.
FastAPI가 GraphService, PriceService, ChatService를 통해 데이터를 제공하고,
React 프론트엔드가 이를 사용자에게 보여줍니다.

--- Airflow에서 유용했던 기능들 ---

처음 써본 입장에서 특히 유용했던 기능 몇 가지를 꼽겠습니다.

첫째, 자동 재시도입니다.
KRX API가 간헐적으로 실패하는 경우가 있는데,
retries=3, retry_delay=timedelta(minutes=5)만 설정하면
5분 간격으로 3번까지 자동 재시도합니다.
cron이었다면 실패 감지, 대기, 재시도를 전부 스크립트에 구현해야 합니다.

둘째, XCom을 통한 Task 간 데이터 전달입니다.
앞선 Task의 결과를 뒤따르는 Task에서 사용할 수 있습니다.
예를 들어 "거래일 목록 조회" Task가 날짜 리스트를 반환하면,
"ETF 수집" Task가 그 리스트를 받아서 해당 날짜들의 데이터를 수집합니다.
Task를 독립적으로 유지하면서도 데이터를 주고받을 수 있는 구조입니다.

셋째, 웹 UI입니다.
각 DAG의 실행 이력, 개별 Task의 성공/실패 상태, 실행 시간, 로그를
브라우저에서 바로 확인할 수 있습니다.
특히 실패한 Task만 골라서 재실행하는 기능이 유용했습니다.
백필 DAG처럼 6시간짜리 작업에서 중간에 하나만 실패했을 때,
처음부터 다시 돌리지 않고 실패한 지점부터 재실행할 수 있습니다.

넷째, 병렬 실행입니다.
의존 관계가 없는 Task들은 자동으로 병렬 실행됩니다.
일별 동기화 DAG에서 보유종목 수집, 주식 가격 수집, 수익률 계산이
동시에 돌아가면서 전체 소요 시간이 줄어듭니다.
이걸 cron으로 구현하려면 백그라운드 프로세스 관리를 직접 해야 합니다.

다섯째, Configuration as Code입니다.
DAG 자체가 Python 코드이기 때문에 사용자가 직접 수집 로직을 작성할 수 있습니다.
pykrx로 보유종목을 파싱하거나, yfinance로 가격을 가져오는 등
Python 라이브러리를 그대로 호출할 수 있어서 자유도가 높습니다.
코드이니까 Git 버전 관리와 코드 리뷰도 자연스럽게 됩니다.

--- 그래프 데이터 모델링 ---

이 프로젝트에서 설계한 그래프 모델을 보겠습니다.

노드는 6종류입니다.
- ETF: 코드, 이름, 순자산, 보수율, 수익률
- Stock: 종목 코드, 이름
- Price: 일자별 시가/고가/저가/종가/거래량
- Company: 운용사 이름
- Tag: 테마 태그 (반도체, AI, 2차전지 등)
- User: 사용자 (워치리스트용)

관계(엣지)는 5종류입니다.
- ETF -[:HOLDS {weight, date}]-> Stock: ETF의 보유종목과 비중
- ETF/Stock -[:HAS_PRICE]-> Price: 가격 이력
- ETF -[:MANAGED_BY]-> Company: 운용사
- ETF -[:TAGGED]-> Tag: 테마 분류
- User -[:WATCHES]-> ETF: 사용자 관심 ETF

이 모델의 장점은 관계 탐색이 직관적이라는 겁니다.
"삼성전자를 보유한 ETF 중에서 반도체 태그가 달린 것"
같은 질의를 MATCH 패턴 하나로 표현할 수 있습니다.

--- Cypher 쿼리 실전 예시 ---

실제로 쓰이는 쿼리 몇 가지를 보겠습니다.

첫 번째, 유사 ETF 찾기입니다.
두 ETF의 공통 보유종목에서 비중의 최솟값을 합산해 유사도를 계산합니다.

  MATCH (e1:ETF {code: '069500'})-[h1:HOLDS]->(s:Stock)
  MATCH (e2:ETF)-[h2:HOLDS]->(s) WHERE e1 <> e2
  WITH e2, COUNT(s) as overlap,
       SUM(CASE WHEN h1.weight < h2.weight THEN h1.weight ELSE h2.weight END) as similarity
  RETURN {code: e2.code, name: e2.name, overlap: overlap, similarity: similarity}
  ORDER BY similarity DESC LIMIT 5

관계형 DB였다면 자기 조인에 GROUP BY에 HAVING까지 복잡해질 쿼리가,
그래프에서는 MATCH 패턴으로 자연스럽게 표현됩니다.

두 번째, 보유종목 변동 감지입니다.
특정 ETF의 이번 주와 지난주 보유종목을 비교해서
신규 편입, 제외, 비중 변화를 추출합니다.

  -- 특정 날짜의 보유종목 조회
  MATCH (e:ETF {code: '069500'})-[h:HOLDS]->(s:Stock)
  WHERE h.date <= '2026-02-14'
  WITH DISTINCT h.date as d
  ORDER BY d DESC LIMIT 1

이렇게 기준일과 비교일 각각의 HOLDS 관계를 조회한 뒤,
Python에서 두 시점의 종목별 비중을 비교해서
added, removed, increased, decreased로 분류합니다.
두 시점의 HOLDS 관계를 매칭하면 깔끔하게 처리됩니다.

--- ETF 탐색 ---

그래프 데이터가 어떻게 활용되는지 보겠습니다.

태그 페이지에서는 반도체, AI, 2차전지 같은 테마별로 ETF를 볼 수 있습니다.
이건 (ETF)-[:TAGGED]->(Tag) 관계를 조회하는 겁니다.

ETF 상세 페이지에서는 보유종목 TOP 10을 보여줍니다.
(ETF)-[:HOLDS]->(Stock) 관계에서 최신 날짜의 비중을 가져옵니다.

유사 ETF 기능은 두 ETF 간 보유종목의 비중 유사도를 계산합니다.
단순히 겹치는 종목 수만 세는 게 아니라,
공통 종목의 비중 중 작은 값을 합산해서 유사도를 구합니다.
비중까지 고려하니까 실제로 비슷한 구성의 ETF를 찾을 수 있습니다.

이런 관계 탐색 쿼리들이 그래프 DB의 진가를 보여주는 부분입니다.

--- 보유종목 변동 감지 ---

워치리스트의 핵심 기능인 보유종목 변동 감지를 보겠습니다.

사용자가 관심 ETF를 등록하면,
일별/주별/월별로 보유종목 비중 변화를 추적합니다.

그래프에서 두 시점의 HOLDS 관계를 비교해서
신규 편입, 제외, 비중 증가, 비중 감소를 분류합니다.

예를 들어 KODEX 반도체가 SK하이닉스 비중을 15%에서 18%로 올렸다면,
"비중 증가 +3%p"로 표시됩니다.

3%p 이상 변동이 있으면 Discord로 알림도 보냅니다.
이건 관리자 User 노드의 WATCHES 관계를 타고 알림 대상을 찾습니다.

--- AI 챗봇 — 전체 구조 ---

챗봇에 smolagents를 어떻게 적용했는지 보겠습니다.

ChatService의 구조는 단순합니다.
smolagents의 CodeAgent가 ReAct 방식으로 질문을 처리하고,
pgvector 기반 Few-shot 코드 예시가 정확도를 보완합니다.

모델은 LiteLLM을 통해 GPT-4.1-mini를 사용하고,
10개의 커스텀 Tool이 등록되어 있습니다.

도구는 크게 세 카테고리입니다.
검색: ETF 검색, 종목 검색, 태그 목록
정보: ETF 상세, ETF 가격, 주식 가격, 보유종목 변동
분석: 유사 ETF, ETF 비교, 직접 Cypher 쿼리

모든 도구가 내부적으로 Apache AGE 그래프를 조회합니다.
두 기술(Apache AGE, smolagents)이 여기서 만납니다.

--- AI 챗봇 — CodeAgent 동작 흐름 ---

이 도구들을 CodeAgent가 어떻게 활용하는지 보겠습니다.

1단계: 사용자가 "삼성전자를 가장 많이 보유한 ETF는?" 이라고 질문합니다.
2단계: 에이전트가 Python 코드를 생성합니다.
  result = stock_search("삼성전자")
  stock_code = json.loads(result)[0]["code"]
  holdings = graph_query("MATCH (e:ETF)-[h:HOLDS]->(s:Stock {code: '" + stock_code + "'}) ...")
3단계: 생성된 코드가 실제로 실행되면서 도구가 호출됩니다.
4단계: 실행 결과를 바탕으로 최종 답변을 생성합니다.

이 과정이 최대 10스텝까지 반복될 수 있습니다.
중간에 에러가 나면 에이전트가 스스로 코드를 수정해서 재시도합니다.
이게 JSON 기반 도구 호출 대비 CodeAgent의 장점입니다.
변수에 값을 저장하고, 조건 분기도 가능하니까요.

실행 과정은 스트리밍으로 프론트엔드에 실시간 표시됩니다.
챗봇이 어떤 도구를 호출하고, 어떤 코드를 실행했는지
접을 수 있는 패널로 보여줍니다.
사용자 입장에서는 AI가 어떤 과정을 거쳐 답을 만들었는지
투명하게 볼 수 있어서 신뢰도가 높아집니다.

--- AI 챗봇 — Tool 정의 ---

이 도구들을 실제로 어떻게 정의하는지 보겠습니다.

smolagents에서 도구를 정의하는 방법은 단순합니다.
Tool 클래스를 상속받아 다섯 가지만 정의하면 됩니다.
name, description, inputs, output_type, 그리고 forward() 메서드입니다.

  class ETFSearchTool(Tool):
      name = "etf_search"
      description = "ETF를 이름이나 코드로 검색합니다"
      inputs = {
          "query": {"type": "string", "description": "검색어"},
          "limit": {"type": "integer", "description": "결과 수"}
      }
      output_type = "string"

      def forward(self, query: str, limit: int = 10) -> str:
          graph = GraphService(self.db)
          results = graph.search_etfs(query, limit)
          return json.dumps(results, ensure_ascii=False)

이게 전부입니다.
여기서 중요한 건 description입니다.
에이전트가 이 설명을 보고 어떤 도구를 호출할지 결정하기 때문입니다.

--- AI 챗봇 — Few-shot 학습 ---

챗봇의 정확도를 높이기 위해 Few-shot 학습을 적용했습니다.

25개의 Python 코드 예시를 사전에 준비했습니다.
이 예시들은 pgvector에 임베딩으로 저장됩니다.

사용자가 질문하면, 질문과 유사한 3개의 코드 예시를 벡터 검색으로 찾아서
프롬프트에 주입합니다.

"반도체 ETF 3개 비교해줘"라고 물으면
비슷한 비교 패턴의 Python 코드 예시가 컨텍스트로 들어가서
에이전트가 도구를 어떻게 조합해야 하는지 참고할 수 있습니다.

CodeAgent가 Python 코드를 생성하는 주체이므로,
예시도 Python 코드인 것이 자연스럽습니다.

이게 pgvector와 Apache AGE를 함께 쓰는 재미있는 조합입니다.
같은 PostgreSQL 안에서 벡터 검색과 그래프 쿼리를 함께 활용됩니다.

--- AI 챗봇 — 설계 변천사 ---

지금의 구조가 처음부터 이랬던 건 아닙니다.
몇 번의 시행착오를 거쳐 도달한 결과입니다.

처음에는 CodeAgent의 ReAct 패턴만 사용했습니다.
단순한 질문에는 잘 동작했지만,
에이전트가 생성하는 Cypher 쿼리의 정확도가 낮았습니다.
AGE의 Cypher 문법이 Neo4j와 미묘하게 다른데,
에이전트가 이런 차이를 알 리가 없으니까요.

그래서 Cypher 쿼리 100개를 Few-shot 예시로 준비하고,
pgvector로 유사한 예시를 찾아 프롬프트에 주입했습니다.
"삼성전자를 많이 보유한 ETF"라고 물으면
비슷한 Cypher 예시 3개가 컨텍스트로 들어가서
에이전트가 AGE에 맞는 Cypher를 생성할 수 있게 됐습니다.

하지만 새로운 문제가 생겼습니다.
"반도체 ETF 3개의 수익률, 보유종목, 가격을 모두 비교해줘"
같은 복잡한 질문에서 에이전트가 방향을 잃었습니다.
도구 호출이 7~8번 필요한데,
중간에 불필요한 호출을 반복하거나 멈추는 경우가 있었습니다.

이 문제를 해결하기 위해 앞단에 분류기를 두었습니다.
LLM이 질문을 분석해서 예상 도구 호출 횟수를 판단하고,
3회 이하면 기존 ReAct로, 4회 이상이면
Plan-Execute-Summarize(PES) 방식으로 전환합니다.

Plan-Execute-Summarize는 세 단계입니다.
먼저 LLM이 실행 계획을 JSON으로 생성하고,
계획대로 도구를 순차 실행한 뒤,
수집된 결과를 종합해서 최종 답변을 만듭니다.
복잡한 질의에서 안정적인 결과를 얻을 수 있었습니다.

그런데 여기서 한 가지 의문이 들었습니다.

smolagents의 CodeAgent는 Python 코드를 직접 생성합니다.
for 루프를 돌 수 있고, 변수에 값을 저장할 수 있고,
조건 분기도 가능합니다.
그렇다면 외부 플래너가 왜 필요한가?

Plan-Execute-Summarize는 JSON으로 도구를 호출하는 에이전트에 필요한 패턴입니다.
도구 호출 사이에 로직을 넣을 수 없으니까
미리 계획을 세워야 하는 겁니다.
하지만 CodeAgent는 코드로 직접 복잡한 로직을 구현할 수 있습니다.
"반도체 ETF 3개 비교"를 위해 계획을 세울 필요 없이,
for 루프 하나면 됩니다.

이 깨달음 이후 Plan-Execute-Summarize를 제거하고 ReAct 단일 경로로 돌아갔습니다.
대신 두 가지를 보강했습니다.

첫째, 시스템 프롬프트에 복잡한 질문 처리 가이드를 추가했습니다.
for 루프, 결과 체이닝, 데이터 정렬 등
코드로 처리하는 패턴을 명시해서 에이전트가 스스로 복잡한 로직을 짜도록 유도했습니다.

둘째, Few-shot 예시를 Cypher 쿼리에서 Python 코드로 전환했습니다.
에이전트가 코드를 생성하는 주체이니까,
예시도 코드여야 자연스럽습니다.
"반도체 ETF 비교" 예시가 Cypher 쿼리 대신
도구를 조합하는 Python 코드로 바뀌었습니다.
구조는 단순해졌지만 복잡한 질문에 대한 대응력은 유지됐습니다.

도구의 본질을 이해하는 것이 중요하다는 교훈을 얻었습니다.
CodeAgent는 "코드를 생성하는 에이전트"입니다.
이 특성을 살리면, 외부 보조 장치 없이도
충분히 복잡한 작업을 수행할 수 있습니다.

--- 포트폴리오 관리 & 대시보드 ---

마지막 주요 기능인 포트폴리오를 보겠습니다.

사용자가 ETF와 주식을 매수 기록하면,
현재 가격 기반으로 포트폴리오 가치를 계산합니다.
목표 비중을 설정하면 실제 비중과의 괴리를 보여줍니다.

대시보드에서는 포트폴리오 가치 변화를 Recharts 차트로 시각화합니다.
portfolio_snapshots 테이블에 일별 스냅샷이 저장되고,
Airflow에서 가격 수집 후 자동으로 스냅샷을 찍습니다.

이 기능은 관계형 테이블을 사용합니다.
모든 것을 그래프에 넣지 않고,
CRUD 중심의 데이터는 관계형으로, 관계 탐색은 그래프로 분리한 설계입니다.


========================================
시행착오
========================================

--- smolagents — 프롬프트 엔지니어링의 중요성 ---

smolagents에서 가장 어려웠던 건 기술이 아니라 프롬프트였습니다.

도구의 description을 어떻게 쓰느냐에 따라
에이전트의 도구 선택 정확도가 크게 달라집니다.

처음에는 "ETF를 검색합니다"처럼 간단하게 썼더니
에이전트가 ETF 검색 대신 직접 Cypher 쿼리를 날리는 경우가 많았습니다.

도구 설명에 "코드나 이름의 일부로 ETF를 검색합니다.
전체 목록이 아닌 특정 ETF를 찾을 때 사용하세요."
처럼 사용 시점과 맥락을 명시하니까 정확도가 올라갔습니다.

시스템 프롬프트도 마찬가지입니다.
"태그 이름을 사용하기 전에 반드시 list_tags를 먼저 호출하세요"
같은 규칙을 명시해야 합니다.
안 그러면 에이전트가 태그 이름을 추측해서 잘못된 쿼리를 만듭니다.

--- 가격 데이터 이전 — RDB에서 AGE로 ---

두 번째 시행착오는 가격 데이터의 저장 위치입니다.

처음에는 etf_prices, stock_prices라는 관계형 테이블에 가격을 저장했습니다.
익숙한 SQL 쿼리로 조회할 수 있고, ORM 지원도 되니까 자연스러운 선택이었습니다.

그런데 챗봇을 만들면서 문제가 생겼습니다.
챗봇의 모든 도구는 GraphService를 통해 AGE 그래프만 조회하는 구조인데,
가격 데이터만 관계형 테이블에 있으니까 SQL과 Cypher를 오가야 했습니다.

가격도 그래프에 넣으면 어떨까 생각했습니다.
ETF나 Stock 노드에서 HAS_PRICE 관계로 Price 노드를 타면
한 번의 그래프 순회로 가격까지 가져올 수 있습니다.
"삼성전자 최근 가격과 이 종목을 보유한 ETF"
같은 복합 질의도 단일 Cypher 쿼리로 해결됩니다.

그래서 Price 노드를 설계하고, HAS_PRICE 관계를 추가하고,
기존 RDB 테이블은 제거했습니다.
가격 서비스는 4단계 폴백 전략을 씁니다.
60초 캐시 → 당일 AGE Price 노드 → yfinance API → AGE 최신 Price 노드.

여기서 얻은 교훈은,
데이터 저장소는 "어디서 주로 조회하느냐"에 맞춰야 한다는 겁니다.
챗봇이 주 소비자라면, 챗봇이 접근하는 저장소에 데이터를 두는 게 자연스럽습니다.

--- 스냅샷과 멱등성 — "오늘"이 도대체 언제인가 ---

포트폴리오 스냅샷은 단순해 보였습니다.
"매일 장 마감 후에 포트폴리오 가치를 계산해서 저장한다."
이게 전부인데, 실제로 만들어보니 "기준일"이라는 개념이 생각보다 복잡했습니다.
기준일과 수집일의 정의가 미흡한 상태에서 출발했더니,
멱등성을 유지하는 데 커밋이 10개 넘게 필요했습니다.

-- 1단계: 메인 DAG에 끼워넣기 --

처음에는 일별 ETF 동기화 DAG의 collect_prices 태스크 뒤에
snapshot_portfolios 태스크를 붙였습니다.
XCom으로 전달받은 trading_date로 스냅샷을 찍는 단순한 구조였습니다.

문제는 이 DAG가 "거래일 다음날 새벽"에 실행된다는 것입니다.
2월 14일(금) 거래 데이터를 2월 15일(토) 새벽에 수집합니다.
스냅샷의 기준일은 2월 14일이어야 하는데,
date.today()를 쓰면 2월 15일이 됩니다.
이때는 XCom의 trading_date를 쓰면 되니까 괜찮았습니다.

-- 2단계: 스냅샷 DAG 분리 --

메인 DAG가 무거워지면서 스냅샷을 별도 DAG로 분리했습니다.
portfolio_snapshot.py를 만들고
평일 16시(장 마감 직후)에 실행되도록 스케줄링했습니다.
이 DAG는 yfinance로 독립적으로 가격을 가져왔습니다.

여기서 새로운 문제가 생겼습니다.
메인 DAG와 스냅샷 DAG가 서로 다른 시간에 돌아가니까,
메인 DAG가 아직 가격을 수집하지 않은 상태에서
스냅샷 DAG가 먼저 실행되는 경우가 있었습니다.
그러면 어제 가격으로 오늘 스냅샷이 찍혀서 데이터가 엉켰습니다.

-- 3단계: 실시간 가격 수집 DAG 등장 --

포트폴리오 현재가를 실시간으로 보여주기 위해
장중 10분마다 가격을 수집하는 새로운 DAG를 만들었습니다.
ticker_prices 테이블에 현재가를 캐싱하는 구조입니다.

이제 가격 소스가 세 곳이 되었습니다.
- AGE Price 노드: 일별 ETF 가격, 장 마감 후 수집
- yfinance API: 실시간 조회
- ticker_prices 테이블: 10분 주기 캐시

스냅샷은 어떤 가격을 기준으로 찍어야 할까?
AGE Price 노드는 전일 종가이고, yfinance는 현재가이고,
ticker_prices는 마지막으로 캐시된 가격입니다.
기준일의 정의가 흔들리기 시작했습니다.

-- 4단계: 스냅샷 DAG 제거와 통합 --

결국 별도 스냅샷 DAG를 제거하고,
실시간 가격 수집 DAG에 스냅샷 로직을 통합했습니다.
collect_prices >> update_snapshots 순서로 실행하면
가격 수집 직후에 스냅샷이 찍히니까 일관성이 보장됩니다.

하지만 장중에 10분마다 스냅샷을 찍으면,
같은 날짜에 스냅샷이 여러 번 생깁니다.
여기서 UNIQUE(portfolio_id, date) 제약 조건과
UPSERT 패턴이 필요해졌습니다.
같은 날짜의 스냅샷은 덮어쓰고, 새로운 날짜면 새로 만듭니다.
이게 멱등성의 핵심이었습니다.

-- 5단계: "오늘"의 함정 — 휴장일과 공휴일 --

그런데 공휴일에서 문제가 생겼습니다.

월요일이 공휴일인 경우를 보겠습니다.
금요일 장 마감 → 토·일 → 월요일(공휴일) → 화요일 장 개장.

실시간 DAG는 평일(월~금)에 10분마다 실행됩니다.
월요일은 평일이지만 공휴일이라 장이 열리지 않습니다.
DAG가 실행되면 check_market_open에서 걸러지지만,
이 판단 로직에서 date.today()를 쓰면
"오늘이 거래일인가?"를 판단해야 하고,
공휴일 캘린더가 필요합니다.

더 미묘한 경우도 있었습니다.
토요일 새벽에 일별 동기화 DAG가 돌면서 금요일 가격을 수집합니다.
그런데 금요일이 공휴일이었다면?
KRX API는 데이터를 반환하지 않습니다.
이때 수익률 계산 태스크가 "새 데이터 없음"을
"수익률 0%"로 처리해버렸습니다.
기존에 있던 1일/1주/1개월 수익률이 전부 0%로 덮어씌워진 것입니다.

sync_returns에 거래일 체크를 추가해서
"새 거래일 데이터가 없으면 기존 수익률을 건드리지 않는다"로 수정했습니다.
데이터가 없는 걸 "0"으로 처리하면 안 되고,
없으면 그냥 아무것도 하지 않아야 합니다.

-- 6단계: 수집일 ≠ 거래일 — KRX API의 반환값 --

비슷한 문제가 KRX API에서도 나왔습니다.
"2월 15일 데이터를 달라"고 요청하면,
15일이 비거래일인 경우 14일 데이터를 반환합니다.
하지만 응답에는 14일이라고 명시되어 있습니다.

문제는 이걸 무시하고
"요청한 날짜 = 데이터의 날짜"로 처리했던 것입니다.
2월 15일로 요청했으니 15일 데이터라고 저장했는데,
실제로는 14일 데이터였습니다.

이후 14일로 다시 요청하면 같은 데이터가 14일자로 들어와서
Price 노드가 중복 생성되었습니다.

해결책은 KRX API가 반환하는 actual_dates를
XCom으로 전파하는 것이었습니다.
"요청한 날짜"가 아니라 "실제 거래가 발생한 날짜"를 기준으로
모든 하위 태스크(holdings, stock_prices, returns, notifications)가
동작하도록 바꿨습니다.

-- 7단계: 수동 트리거의 부작용 --

Airflow의 "DAG 수동 실행" 기능도
멱등성을 깨뜨리는 원인이었습니다.

일별 동기화 DAG를 수동으로 실행하면,
이미 수집된 날짜에 대해 record_collection_run이 다시 실행되고,
Discord 알림이 중복으로 발생했습니다.
INSERT ... ON CONFLICT DO NOTHING으로
DB 쓰기는 멱등적이었지만,
pg_notify와 Discord 알림은
INSERT 성공 여부와 무관하게 항상 발행되고 있었습니다.

수정은 간단했습니다.
INSERT 결과의 rowcount를 확인해서
실제로 새 행이 생겼을 때만 알림을 보내도록 했습니다.

-- 8단계: 수동 스냅샷 적용 --

DAG의 멱등성 문제를 해결해가면서 동시에 생각한 게 있습니다.
사용자 입장에서는 항상 최신 데이터를 보고 싶어 합니다.
DAG가 하루에 한 번 돌든 10분마다 돌든,
사용자가 페이지를 열었을 때 "지금" 기준의 가치를 확인하고 싶은 거죠.

그래서 holdings 변경 시 자동으로 호출되던
스냅샷 갱신 코드를 5군데에서 제거하고,
대신 "스냅샷 적용" 버튼을 만들었습니다.
사용자가 보고 싶을 때 버튼을 누르면
그 시점의 최신 가격으로 스냅샷이 갱신됩니다.

기준일은 date.today() 대신
ticker_prices의 MAX(date)를 사용합니다.
이 값은 마지막으로 가격이 수집된 거래일을 의미하니까,
공휴일이든 주말이든 항상 올바른 거래일을 가리킵니다.

프론트엔드에서는 두 가지 시간을 분리해서 보여줍니다.
- 기준일: "어떤 거래일의 가격으로 계산했는가" (예: 2/14)
- 스냅샷 시각: "언제 계산을 수행했는가" (예: 2/15 10:30)

"오늘"이라는 모호한 개념 대신,
"가격의 기준 시점"과 "계산의 실행 시점"을 분리하니까
자동 수집이든 수동 적용이든 일관되게 동작했습니다.

-- 돌아보면 --

스냅샷 하나 만드는 데 커밋이 10개 넘게 들어갔습니다.
결국 "기준일"과 "수집일"을 명확히 분리하고,
date.today() 대신 데이터가 알려주는 날짜를 쓰고,
데이터가 없을 때는 아무것도 하지 않는 것.
이 세 가지로 정리가 됐는데,
만들면서 하나씩 구체화된 부분이라
처음부터 다 예측하기는 어려웠을 겁니다.


--- 그래프 vs 관계형 — 어디에 뭘 넣을까 ---

처음에는 모든 데이터를 그래프에 넣으려고 했습니다.
하지만 실제로 해보니, 적합한 영역이 나뉩니다.

그래프에 적합한 것:
- 관계 탐색 (ETF-종목 보유, 유사 ETF, 태그 분류)
- 시계열 + 관계 (날짜별 보유종목 비중 변화)
- 다대다 관계 (사용자-ETF 워치리스트)

관계형에 적합한 것:
- 단순 CRUD (사용자 정보, 포트폴리오, 매수 기록)
- 집계/정렬 중심 조회 (포트폴리오 스냅샷, 가치 계산)
- 트랜잭션이 중요한 데이터 (인증, 주문)

Apache AGE의 장점이 바로 이 분리를 한 DB에서 할 수 있다는 것입니다.
PostgreSQL 테이블과 AGE 그래프가 같은 트랜잭션 안에서 동작합니다.

결론적으로, "전부 그래프" 또는 "전부 관계형"이 아니라
데이터 특성에 맞게 섞어 쓰는 게 현실적인 접근이었습니다.


========================================
회고 & 마무리
========================================

--- 기술 선택 회고 ---

프로젝트를 마치며 각 기술에 대한 솔직한 평가입니다.

Apache AGE:
- 좋았던 점: PostgreSQL 하나로 관계형 + 그래프 + 벡터 검색까지 가능.
  인프라 복잡도가 획기적으로 줄어듭니다.
- 아쉬운 점: Cypher 지원이 Neo4j 대비 불완전하고,
  ORM 통합 가이드가 거의 없어서 삽질이 많았습니다.
  커뮤니티도 아직 작습니다.
- 추천 상황: 이미 PostgreSQL을 쓰고 있고,
  그래프 쿼리가 전체 워크로드의 일부일 때.

smolagents:
- 좋았던 점: 도구 정의가 직관적이고, CodeAgent의 코드 생성 방식이 유연합니다.
  소스 코드가 읽을 만한 수준으로 작아서 내부 동작을 이해하기 쉽습니다.
- 아쉬운 점: 문서가 부족하고, 고급 기능은 직접 구현해야 합니다.
  프롬프트 의존도가 높아서 튜닝에 시간이 많이 듭니다.
- 추천 상황: 도구 호출 중심의 에이전트를 빠르게 만들고 싶을 때.
  복잡한 RAG 파이프라인이 필요하면 LangChain이 나을 수 있습니다.

Apache Airflow:
- 좋았던 점: 작업 흐름을 Python 코드로 관리할 수 있다는 것 자체가 큰 가치입니다.
  웹 UI에서 실행 상태를 한눈에 볼 수 있고,
  실패한 Task만 골라서 재실행할 수 있어서 운영이 편합니다.
  자동 재시도, 병렬 실행, XCom 데이터 전달 같은 기능이
  별도 구현 없이 설정만으로 동작합니다.
- 아쉬운 점: 초기 설정이 번거롭습니다.
  메타데이터 DB, Executor, 사용자 설정 등 "Hello World"까지 단계가 많습니다.
  다만 Docker로 한번 세팅하면 이후에는 DAG 파일만 관리하면 됩니다.
  NiFi 대비 실시간 처리에는 약하지만, 배치 중심 워크로드에서는 문제없었습니다.
- 추천 상황: Python 기반 배치 ETL, 정기 데이터 수집 파이프라인.
  실시간 데이터 라우팅이 핵심이라면 NiFi가 더 적합합니다.

--- Q&A ---

감사합니다.
질문 받겠습니다.
